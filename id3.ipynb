{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from math import log\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "irisdata = pd.read_csv('iris.data', sep=\",\", names=['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Class'])\n",
    "irisdata = irisdata.drop(\"Class\", 1)\n",
    "winedata = pd.read_csv('wine.data', sep=\",\", names=['Alcohol','MalicAcid','Ash','AlcalinityOfAsh','Magnesium','Total phenols','Flavanoids','NonflavanoidPhenols','Proanthocyanins','ColorIntensity','Hue','OD280/OD315OfDilutedWines','Proline'  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset):\n",
    "    noCol = len (dataset.columns)\n",
    "    for column in dataset:\n",
    "        col = dataset[column]\n",
    "        name = \"new\" + col.name\n",
    "        data = col.values\n",
    "        temp = pd.cut(data, bins = 5)\n",
    "        dataset[name] = temp\n",
    "        \n",
    "    dataset.drop (columns = dataset.columns[:noCol], axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "       \n",
    "  def __init__(self, label):\n",
    " \n",
    "    self.attribute = None  # Attribute (e.g. 'Outlook')\n",
    "    self.attribute_values = []  # Values (e.g. 'Sunny')\n",
    "    self.label = label   # Class label for the node (e.g. 'Play')\n",
    "    self.children = {}   # Keeps track of the node's children\n",
    "     \n",
    "    # References to the parent node\n",
    "    self.parent_attribute = None\n",
    "    self.parent_attribute_value = None\n",
    " \n",
    "    # Used for pruned trees\n",
    "    self.pruned = False  # Is this tree pruned? \n",
    "    self.instances_labeled = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ID3(instances, default):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "      instances: A list of dictionaries where each dictionary is an instance. \n",
    "                 Each dictionary contains attribute:value pairs \n",
    "                 e.g.: instances =\n",
    "                   {'Class':'Play','Outlook':'Sunny','Temperature':'Hot'}\n",
    "                   {'Class':'Don't Play','Outlook':'Rain','Temperature':'Cold'}\n",
    "                   {'Class':'Play','Outlook':'Overcast','Temperature':'Hot'}\n",
    "                   ...\n",
    "                   etc.\n",
    "                 The first attribute:value pair is the \n",
    "                 target variable (i.e. the class of that instance)\n",
    "      default: The default class label (e.g. 'Play')\n",
    "    Returns:\n",
    "      tree: An object of the Node class\n",
    "    \"\"\"   \n",
    "    # The len method returns the number of items in the list\n",
    "    # If there are no more instances left, return a leaf that is labeled with \n",
    "    # the default class\n",
    "    if len(instances) == 0:\n",
    "        return Node(default)\n",
    " \n",
    "    classes = []  # Create an empty list named 'classes'\n",
    " \n",
    "    # For each instance in the list of instances, append the value of the class\n",
    "    # to the end of the classes list\n",
    "    for instance in instances:\n",
    "        classes.append(instance['Class'])\n",
    " \n",
    "    # If all instances have the same class label or there is only one instance\n",
    "    # remaining, create a leaf node labeled with that class. \n",
    "    # Counter(list) creates a tally of each element in the list. This tally is \n",
    "    # represented as an element:tally pair.\n",
    "    if len(Counter(classes)) == 1 or len(classes) == 1:\n",
    "        tree = Node(mode_class(instances))\n",
    "        return tree\n",
    " \n",
    "    # Otherwise, find the best attribute, the attribute that maximizes the gain \n",
    "    # ratio of the data set, to be the next decision node.\n",
    "    else:\n",
    "        # Find the name of the most informative attribute of the data set\n",
    "        # e.g. \"Outlook\"\n",
    "        best_attribute = most_informative_attribute(instances)\n",
    " \n",
    "        # Initialize a tree with the most common class\n",
    "        # e.g. \"Play\"\n",
    "        tree = Node(mode_class(instances))\n",
    " \n",
    "        # The most informative attribute becomes this decision node\n",
    "        # e.g. \"Outlook\" becomes this node\n",
    "        tree.attribute = best_attribute\n",
    " \n",
    "        best_attribute_values = []\n",
    " \n",
    "        # The branches of the node are the values of the best_attribute\n",
    "        # e.g. \"Sunny\", \"Overcast\", \"Rainy\"\n",
    "        # Go through each instance and create a list of the values of \n",
    "        # best_attribute\n",
    "        for instance in instances:\n",
    "            try:\n",
    "                best_attribute_values.append(instance[best_attribute])\n",
    "            except:\n",
    "                no_best_attribute = True\n",
    "        # Create a list of the unique best attribute values\n",
    "        # Set is like a list except it extracts nonduplicate (unique) \n",
    "        # items from a list. \n",
    "        # In short, we create a list of the set of unique\n",
    "        # best attribute values.\n",
    "        tree.attribute_values = list(set(best_attribute_values))\n",
    " \n",
    "        # Now we need to split the instances. We will create separate subsets\n",
    "        # for each best attribute value. These become the child nodes\n",
    "        # i.e. \"Sunny\", \"Overcast\", \"Rainy\" subsets\n",
    "        for best_attr_value_i in tree.attribute_values:\n",
    " \n",
    "            # Generate the subset of instances\n",
    "            instances_i = []\n",
    "            # Go through one instance at a time\n",
    "            for instance in instances:\n",
    "                # e.g. If this instance has \"Sunny\" as its best attribute value\n",
    "                if instance[best_attribute] == best_attr_value_i:\n",
    "                    instances_i.append(instance) #Add this instance to the list\n",
    " \n",
    "            # Create a subtree recursively\n",
    "            subtree = ID3(instances_i, mode_class(instances))\n",
    " \n",
    "            # Initialize the values of the subtree\n",
    "            subtree.instances_labeled = instances_i\n",
    " \n",
    "            # Keep track of the state of the subtree's parent (i.e. tree)\n",
    "            subtree.parent_attribute = best_attribute # parent node\n",
    "            subtree.parent_attribute_value = best_attr_value_i # branch name\n",
    " \n",
    "            # Assign the subtree to the appropriate branch\n",
    "            tree.children[best_attr_value_i] = subtree\n",
    " \n",
    "        # Return the decision tree\n",
    "        return tree\n",
    " \n",
    " \n",
    "def mode_class(instances):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "      instances: A list of dictionaries where each dictionary is an instance. \n",
    "        Each dictionary contains attribute:value pairs \n",
    "    Returns:\n",
    "      Name of the most common class (e.g. 'Don't Play')\n",
    "    \"\"\"\n",
    " \n",
    "    classes = []  # Create an empty list named 'classes'\n",
    " \n",
    "    # For each instance in the list of instances, append the value of the class\n",
    "    # to the end of the classes list\n",
    "    for instance in instances:\n",
    "        classes.append(instance['Class'])\n",
    " \n",
    "    # The 1 ensures that we get the top most common class\n",
    "    # The [0][0] ensures we get the name of the class label and not the tally\n",
    "    # Return the name of the most common class of the instances\n",
    "    return Counter(classes).most_common(1)[0][0]\n",
    " \n",
    "def prior_entropy(instances):\n",
    "    \"\"\"\n",
    "    Calculate the entropy of the data set with respect to the actual class\n",
    "    prior to splitting the data.\n",
    "    Parameters:\n",
    "      instances: A list of dictionaries where each dictionary is an instance. \n",
    "        Each dictionary contains attribute:value pairs \n",
    "    Returns:\n",
    "      Entropy value in bits\n",
    "    \"\"\"\n",
    "    # For each instance in the list of instances, append the value of the class\n",
    "    # to the end of the classes list    \n",
    "    classes = []  # Create an empty list named 'classes'\n",
    " \n",
    "    for instance in instances:\n",
    "        classes.append(instance['Class'])\n",
    "    counter = Counter(classes)\n",
    " \n",
    "    # If all instances have the same class, the entropy is 0\n",
    "    if len(counter) == 1:\n",
    "        return 0\n",
    "    else:\n",
    "    # Compute the weighted sum of the logs of the probabilities of each \n",
    "    # possible outcome \n",
    "        entropy = 0\n",
    "        for c, count_of_c in counter.items():\n",
    "            probability = count_of_c / len(classes)\n",
    "            entropy += probability * (log(probability, 2))\n",
    "        return -entropy\n",
    " \n",
    "def entropy(instances, attribute, attribute_value):\n",
    "    \"\"\"\n",
    "    Calculate the entropy for a subset of the data filtered by attribute value\n",
    "    Parameters:\n",
    "      instances: A list of dictionaries where each dictionary is an instance. \n",
    "        Each dictionary contains attribute:value pairs \n",
    "      attribute: The name of the attribute (e.g. 'Outlook')\n",
    "      attribute_value: The value of the attribute (e.g. 'Sunny')\n",
    "    Returns:\n",
    "      Entropy value in bits\n",
    "    \"\"\"\n",
    "    # For each instance in the list of instances, append the value of the class\n",
    "    # to the end of the classes list    \n",
    "    classes = []  # Create an empty list named 'classes'\n",
    " \n",
    "    for instance in instances:\n",
    "        if instance[attribute] == attribute_value:\n",
    "            classes.append(instance['Class'])\n",
    "    counter = Counter(classes)\n",
    " \n",
    "    # If all instances have the same class, the entropy is 0\n",
    "    if len(counter) == 1:\n",
    "        return 0\n",
    "    else:\n",
    "    # Compute the weighted sum of the logs of the probabilities of each \n",
    "    # possible outcome \n",
    "        entropy = 0\n",
    "        for c, count_of_c in counter.items():\n",
    "            probability = count_of_c / len(classes)\n",
    "            entropy += probability * (log(probability, 2))\n",
    "        return -entropy\n",
    " \n",
    "def gain_ratio(instances, attribute):\n",
    "    \"\"\"\n",
    "    Calculate the gain ratio if we were to split the data set based on the values\n",
    "    of this attribute.\n",
    "    Parameters:\n",
    "      instances: A list of dictionaries where each dictionary is an instance. \n",
    "        Each dictionary contains attribute:value pairs \n",
    "      attribute: The name of the attribute (e.g. 'Outlook')\n",
    "    Returns:\n",
    "      The gain ratio\n",
    "    \"\"\"\n",
    "    # Record the entropy of the combined set of instances\n",
    "    priorentropy = prior_entropy(instances)\n",
    " \n",
    "    values = []\n",
    " \n",
    "    # Create a list of the attribute values for each instance\n",
    "    for instance in instances:\n",
    "        values.append(instance[attribute])\n",
    "    counter = Counter(values) # Store the frequency counts of each attribute value\n",
    " \n",
    "    # The remaining entropy if we were to split the instances based on this attribute\n",
    "    # This is a weighted entropy score sum\n",
    "    remaining_entropy = 0\n",
    " \n",
    "    # This variable is used for the gain ratio calculation\n",
    "    split_information = 0\n",
    " \n",
    "    # items() method returns a list of all dictionary key-value pairs\n",
    "    for attribute_value, attribute_value_count in counter.items():\n",
    "        probability = attribute_value_count/len(values)\n",
    "        remaining_entropy += (probability * entropy(\n",
    "            instances, attribute, attribute_value))\n",
    "        split_information += probability * (log(probability, 2))\n",
    " \n",
    "    information_gain = priorentropy - remaining_entropy\n",
    " \n",
    "    split_information = -split_information\n",
    " \n",
    "    gainratio = None\n",
    " \n",
    "    if split_information != 0:\n",
    "        gainratio = information_gain / split_information\n",
    "    else:\n",
    "        gainratio = -1000\n",
    " \n",
    "    return gainratio\n",
    " \n",
    "def most_informative_attribute(instances):\n",
    "    \"\"\"\n",
    "    Choose the attribute that provides the most information if you were to\n",
    "    split the data set based on that attribute's values. This attribute is the \n",
    "    one that has the highest gain ratio.\n",
    "    Parameters:\n",
    "      instances: A list of dictionaries where each dictionary is an instance. \n",
    "        Each dictionary contains attribute:value pairs \n",
    "      attribute: The name of the attribute (e.g. 'Outlook')\n",
    "    Returns:\n",
    "      The name of the most informative attribute\n",
    "    \"\"\"\n",
    "    selected_attribute = None\n",
    "    max_gain_ratio = -1000\n",
    " \n",
    "    # instances[0].items() extracts the first instance in instances\n",
    "    # for key, value iterates through each key-value pair in the first\n",
    "    # instance in instances\n",
    "    # In short, this code creates a list of the attribute names\n",
    "    attributes = [key for key, value in instances[0].items()]\n",
    "    # Remove the \"Class\" attribute name from the list\n",
    "    attributes.remove('Class')\n",
    " \n",
    "    # For every attribute in the list of attributes\n",
    "    for attribute in attributes:\n",
    "        # Calculate the gain ratio and store that value\n",
    "        gain = gain_ratio(instances, attribute)\n",
    " \n",
    "        # If we have a new most informative attribute\n",
    "        if gain > max_gain_ratio:\n",
    "            max_gain_ratio = gain\n",
    "            selected_attribute = attribute\n",
    " \n",
    "    return selected_attribute\n",
    " \n",
    "def accuracy(trained_tree, test_instances):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        trained_tree: A tree that has already been trained\n",
    "        test_instances: A set of test instances\n",
    "    Returns:\n",
    "        Classification accuracy (# of correct predictions/# of predictions) \n",
    "    \"\"\"\n",
    "    # Set the counter to 0\n",
    "    no_of_correct_predictions = 0\n",
    " \n",
    "    for test_instance in test_instances:\n",
    "        if predict(trained_tree, test_instance) == test_instance['Class']:\n",
    "            no_of_correct_predictions += 1\n",
    " \n",
    "    return no_of_correct_predictions / len(test_instances)\n",
    " \n",
    "def predict(node, test_instance):\n",
    "    '''\n",
    "    Parameters:\n",
    "        node: A trained tree node\n",
    "        test_instance: A single test instance\n",
    "    Returns:\n",
    "        Class value (e.g. \"Play\")\n",
    "    '''\n",
    "    # Stopping case for the recursive call.\n",
    "    # If this is a leaf node (i.e. has no children)\n",
    "    if len(node.children) == 0:\n",
    "        return node.label\n",
    "    # Otherwise, we are not yet on a leaf node.\n",
    "    # Call predict method recursively until we get to a leaf node.\n",
    "    else:\n",
    "        # Extract the attribute name (e.g. \"Outlook\") from the node. \n",
    "        # Record the value of the attribute for this test instance into \n",
    "        # attribute_value (e.g. \"Sunny\")\n",
    "        attribute_value = test_instance[node.attribute]\n",
    " \n",
    "        # Follow the branch for this attribute value assuming we have \n",
    "        # an unpruned tree.\n",
    "        if attribute_value in node.children and node.children[\n",
    "            attribute_value].pruned == False:\n",
    "            # Recursive call\n",
    "            return predict(node.children[attribute_value], test_instance)\n",
    " \n",
    "        # Otherwise, return the most common class\n",
    "        # return the mode label of examples with other attribute values for the current attribute\n",
    "        else:\n",
    "            instances = []\n",
    "            for attr_value in node.attribute_values:\n",
    "                instances += node.children[attr_value].instances_labeled\n",
    "            return mode_class(instances)\n",
    " \n",
    "TREE = None\n",
    "def prune(node, val_instances):\n",
    "    \"\"\"\n",
    "    Prune the tree recursively, starting from the leaves\n",
    "    Parameters:\n",
    "        node: A tree that has already been trained\n",
    "        val_instances: The validation set        \n",
    "    \"\"\"\n",
    "    global TREE\n",
    "    TREE = node\n",
    " \n",
    "    def prune_node(node, val_instances):\n",
    "        # If this is a leaf node\n",
    "        if len(node.children) == 0:\n",
    "            accuracy_before_pruning = accuracy(TREE, val_instances)\n",
    "            node.pruned = True\n",
    " \n",
    "            # If no improvement in accuracy, no pruning\n",
    "            if accuracy_before_pruning >= accuracy(TREE, val_instances):\n",
    "                node.pruned = False\n",
    "            return\n",
    " \n",
    "        for value, child_node in node.children.items():\n",
    "            prune_node(child_node, val_instances)\n",
    " \n",
    "        # Prune when we reach the end of the recursion\n",
    "        accuracy_before_pruning = accuracy(TREE, val_instances)\n",
    "        node.pruned = True\n",
    " \n",
    "        if accuracy_before_pruning >= accuracy(TREE, val_instances):\n",
    "            node.pruned = False\n",
    " \n",
    "    prune_node(TREE, val_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    newSepalLength newSepalWidth newPetalLength   newPetalWidth\n",
      "0     (5.02, 5.74]  (3.44, 3.92]  (0.994, 2.18]  (0.0976, 0.58]\n",
      "1    (4.296, 5.02]  (2.96, 3.44]  (0.994, 2.18]  (0.0976, 0.58]\n",
      "2    (4.296, 5.02]  (2.96, 3.44]  (0.994, 2.18]  (0.0976, 0.58]\n",
      "3    (4.296, 5.02]  (2.96, 3.44]  (0.994, 2.18]  (0.0976, 0.58]\n",
      "4    (4.296, 5.02]  (3.44, 3.92]  (0.994, 2.18]  (0.0976, 0.58]\n",
      "..             ...           ...            ...             ...\n",
      "145   (6.46, 7.18]  (2.96, 3.44]   (4.54, 5.72]     (2.02, 2.5]\n",
      "146   (5.74, 6.46]  (2.48, 2.96]   (4.54, 5.72]    (1.54, 2.02]\n",
      "147   (6.46, 7.18]  (2.96, 3.44]   (4.54, 5.72]    (1.54, 2.02]\n",
      "148   (5.74, 6.46]  (2.96, 3.44]   (4.54, 5.72]     (2.02, 2.5]\n",
      "149   (5.74, 6.46]  (2.96, 3.44]   (4.54, 5.72]    (1.54, 2.02]\n",
      "\n",
      "[150 rows x 4 columns]\n",
      "        newAlcohol    newMalicAcid          newAsh newAlcalinityOfAsh  \\\n",
      "1   (14.07, 14.83]  (0.735, 1.752]  (2.108, 2.482]     (14.48, 18.36]   \n",
      "1   (12.55, 13.31]  (1.752, 2.764]  (2.108, 2.482]    (10.581, 14.48]   \n",
      "1   (12.55, 13.31]  (1.752, 2.764]  (2.482, 2.856]     (18.36, 22.24]   \n",
      "1   (14.07, 14.83]  (1.752, 2.764]  (2.482, 2.856]     (14.48, 18.36]   \n",
      "1   (12.55, 13.31]  (1.752, 2.764]   (2.856, 3.23]     (18.36, 22.24]   \n",
      "..             ...             ...             ...                ...   \n",
      "3   (13.31, 14.07]    (4.788, 5.8]  (2.108, 2.482]     (18.36, 22.24]   \n",
      "3   (13.31, 14.07]  (3.776, 4.788]  (2.108, 2.482]     (22.24, 26.12]   \n",
      "3   (12.55, 13.31]  (3.776, 4.788]  (2.108, 2.482]     (18.36, 22.24]   \n",
      "3   (12.55, 13.31]  (1.752, 2.764]  (2.108, 2.482]     (18.36, 22.24]   \n",
      "3   (14.07, 14.83]  (3.776, 4.788]  (2.482, 2.856]     (22.24, 26.12]   \n",
      "\n",
      "      newMagnesium newTotal phenols   newFlavanoids newNonflavanoidPhenols  \\\n",
      "1   (125.2, 143.6]      (2.72, 3.3]  (2.236, 3.184]         (0.236, 0.342]   \n",
      "1    (88.4, 106.8]     (2.14, 2.72]  (2.236, 3.184]         (0.236, 0.342]   \n",
      "1    (88.4, 106.8]      (2.72, 3.3]  (3.184, 4.132]         (0.236, 0.342]   \n",
      "1   (106.8, 125.2]      (3.3, 3.88]  (3.184, 4.132]         (0.236, 0.342]   \n",
      "1   (106.8, 125.2]      (2.72, 3.3]  (2.236, 3.184]         (0.342, 0.448]   \n",
      "..             ...              ...             ...                    ...   \n",
      "3    (88.4, 106.8]     (1.56, 2.14]  (0.335, 1.288]         (0.448, 0.554]   \n",
      "3    (88.4, 106.8]     (1.56, 2.14]  (0.335, 1.288]         (0.342, 0.448]   \n",
      "3   (106.8, 125.2]     (1.56, 2.14]  (0.335, 1.288]         (0.342, 0.448]   \n",
      "3   (106.8, 125.2]     (1.56, 2.14]  (0.335, 1.288]         (0.448, 0.554]   \n",
      "3    (88.4, 106.8]     (1.56, 2.14]  (0.335, 1.288]          (0.554, 0.66]   \n",
      "\n",
      "   newProanthocyanins newColorIntensity          newHue  \\\n",
      "1      (1.678, 2.312]    (3.624, 5.968]  (0.972, 1.218]   \n",
      "1      (1.044, 1.678]    (3.624, 5.968]  (0.972, 1.218]   \n",
      "1      (2.312, 2.946]    (3.624, 5.968]  (0.972, 1.218]   \n",
      "1      (1.678, 2.312]    (5.968, 8.312]  (0.726, 0.972]   \n",
      "1      (1.678, 2.312]    (3.624, 5.968]  (0.972, 1.218]   \n",
      "..                ...               ...             ...   \n",
      "3      (1.044, 1.678]    (5.968, 8.312]  (0.479, 0.726]   \n",
      "3      (1.044, 1.678]    (5.968, 8.312]  (0.479, 0.726]   \n",
      "3      (1.044, 1.678]   (8.312, 10.656]  (0.479, 0.726]   \n",
      "3      (1.044, 1.678]   (8.312, 10.656]  (0.479, 0.726]   \n",
      "3      (1.044, 1.678]   (8.312, 10.656]  (0.479, 0.726]   \n",
      "\n",
      "   newOD280/OD315OfDilutedWines        newProline  \n",
      "1                  (3.454, 4.0]   (838.8, 1119.2]  \n",
      "1                (2.908, 3.454]   (838.8, 1119.2]  \n",
      "1                (2.908, 3.454]  (1119.2, 1399.6]  \n",
      "1                (2.908, 3.454]  (1399.6, 1680.0]  \n",
      "1                (2.908, 3.454]    (558.4, 838.8]  \n",
      "..                          ...               ...  \n",
      "3                (1.267, 1.816]    (558.4, 838.8]  \n",
      "3                (1.267, 1.816]    (558.4, 838.8]  \n",
      "3                (1.267, 1.816]    (558.4, 838.8]  \n",
      "3                (1.267, 1.816]   (838.8, 1119.2]  \n",
      "3                (1.267, 1.816]    (558.4, 838.8]  \n",
      "\n",
      "[178 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "preprocess(irisdata)\n",
    "preprocess(winedata)\n",
    "print(irisdata)\n",
    "print(winedata)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cba033dbfb308e6ef82904202bf1aac94d65a342a0fa0b8648c060ba2fe22e31"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
